{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2SpE5hU8t8QZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fB7As0jZud34"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "0wVvIYnuutec",
    "outputId": "2d7b7ca3-5f9c-4ad4-94bd-6e19f197618c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "cifar_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "K34CLS4Zu1oO",
    "outputId": "4bdafef0-60a8-4f95-cf9f-be5e55fb03f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(cifar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "-R5ZgU7IvAXE",
    "outputId": "5108272a-8610-4ec9-842e-bfbd6a71020e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFSGWMkfvCaq"
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "vAUveK7CvjeY",
    "outputId": "c1c709db-78ee-4fef-97e3-98207122c11b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "images, labels = next(iter(trainloader))\n",
    "print(images.shape, labels.shape)\n",
    "# grid = torchvision.utils.make_grid(images)\n",
    "# grid = torch.transpose(grid, [1,2,0])\n",
    "# plt.imshow(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x1X5WrKTvMQD"
   },
   "source": [
    "#代码练习部分，使用pytorch构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QiTOHpdXvHdT"
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    # 一般在__init__中定义网络需要的操作算子，比如卷积、全连接算子等等\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.fc1 = nn.Linear(4*4*64, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    # 定义了前向传播的运算，只需要像写普通的python算数运算那样就可以了\n",
    "    def forward(self, x):\n",
    "        #进行代码练习\n",
    "        in_size = x.size(0)\n",
    "        out = self.bn1(self.conv1(x))\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out,2,2)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out,2,2)\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = F.relu(out)\n",
    "        out = F.max_pool2d(out,2,2)\n",
    "        out = out.view(in_size, -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        out = F.log_softmax(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOLajeFVvPhN"
   },
   "outputs": [],
   "source": [
    "# 如果你没有GPU，那么可以忽略device相关的代码\n",
    "# device = torch.device(\"cuda:0\")\n",
    "# net = LeNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWm56QyFvTXa"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# CrossEntropyLoss就是我们需要的损失函数\n",
    "net = LeNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "colab_type": "code",
    "id": "zjQkevHQzmSC",
    "outputId": "ce536a9b-a49a-4fd4-b6f4-96e90837ed27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0tRGyiIcIDFN"
   },
   "outputs": [],
   "source": [
    "def evaluate(epoch):\n",
    "  # 构造测试的dataloader\n",
    "  dataiter = iter(testloader)\n",
    "  # 预测正确的数量和总数量\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  # 使用torch.no_grad的话在前向传播中不记录梯度，节省内存\n",
    "  with torch.no_grad():\n",
    "      for data in testloader:\n",
    "          images, labels = data\n",
    "          #images, labels = images.to(device), labels.to(device)\n",
    "          # 预测\n",
    "          outputs = net(images)\n",
    "          # 我们的网络输出的实际上是个概率分布，去最大概率的哪一项作为预测分类\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += labels.size(0)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "  print('[{}] acc:{}%'.format(epoch, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LhFwXOh5vU90",
    "outputId": "34383e73-f2d2-4dae-e12a-432de9148cb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "[Epoch 2, Batch   500] loss: 8.564\n",
      "[Epoch 2, Batch  1000] loss: 6.575\n",
      "[Epoch 2, Batch  1500] loss: 6.003\n",
      "[1] acc:60.24%\n",
      "[Epoch 3, Batch   500] loss: 5.353\n",
      "[Epoch 3, Batch  1000] loss: 5.126\n",
      "[Epoch 3, Batch  1500] loss: 4.962\n",
      "[2] acc:65.42%\n",
      "[Epoch 4, Batch   500] loss: 4.597\n",
      "[Epoch 4, Batch  1000] loss: 4.509\n",
      "[Epoch 4, Batch  1500] loss: 4.405\n",
      "[3] acc:67.97%\n",
      "[Epoch 5, Batch   500] loss: 4.124\n",
      "[Epoch 5, Batch  1000] loss: 4.089\n",
      "[Epoch 5, Batch  1500] loss: 4.049\n",
      "[4] acc:69.85%\n",
      "[Epoch 6, Batch   500] loss: 3.806\n",
      "[Epoch 6, Batch  1000] loss: 3.740\n",
      "[Epoch 6, Batch  1500] loss: 3.829\n",
      "[5] acc:71.36%\n",
      "[Epoch 7, Batch   500] loss: 3.489\n",
      "[Epoch 7, Batch  1000] loss: 3.531\n",
      "[Epoch 7, Batch  1500] loss: 3.541\n",
      "[6] acc:72.32%\n",
      "[Epoch 8, Batch   500] loss: 3.316\n",
      "[Epoch 8, Batch  1000] loss: 3.327\n",
      "[Epoch 8, Batch  1500] loss: 3.310\n",
      "[7] acc:72.79%\n",
      "[Epoch 9, Batch   500] loss: 3.030\n",
      "[Epoch 9, Batch  1000] loss: 3.130\n",
      "[Epoch 9, Batch  1500] loss: 3.190\n",
      "[8] acc:74.14%\n",
      "[Epoch 10, Batch   500] loss: 2.893\n",
      "[Epoch 10, Batch  1000] loss: 2.889\n",
      "[Epoch 10, Batch  1500] loss: 3.011\n",
      "[9] acc:73.59%\n",
      "[Epoch 11, Batch   500] loss: 2.718\n",
      "[Epoch 11, Batch  1000] loss: 2.758\n",
      "[Epoch 11, Batch  1500] loss: 2.795\n",
      "[10] acc:74.08%\n",
      "[Epoch 12, Batch   500] loss: 2.537\n",
      "[Epoch 12, Batch  1000] loss: 2.663\n",
      "[Epoch 12, Batch  1500] loss: 2.626\n",
      "[11] acc:73.81%\n",
      "[Epoch 13, Batch   500] loss: 2.440\n",
      "[Epoch 13, Batch  1000] loss: 2.532\n",
      "[Epoch 13, Batch  1500] loss: 2.444\n",
      "[12] acc:74.84%\n",
      "[Epoch 14, Batch   500] loss: 2.201\n",
      "[Epoch 14, Batch  1000] loss: 2.309\n",
      "[Epoch 14, Batch  1500] loss: 2.449\n",
      "[13] acc:74.41%\n",
      "[Epoch 15, Batch   500] loss: 2.175\n",
      "[Epoch 15, Batch  1000] loss: 2.230\n",
      "[Epoch 15, Batch  1500] loss: 2.220\n",
      "[14] acc:74.35%\n",
      "[Epoch 16, Batch   500] loss: 1.932\n",
      "[Epoch 16, Batch  1000] loss: 2.111\n",
      "[Epoch 16, Batch  1500] loss: 2.153\n",
      "[15] acc:75.09%\n",
      "[Epoch 17, Batch   500] loss: 1.861\n",
      "[Epoch 17, Batch  1000] loss: 1.950\n",
      "[Epoch 17, Batch  1500] loss: 2.045\n",
      "[16] acc:74.92%\n",
      "[Epoch 18, Batch   500] loss: 1.707\n",
      "[Epoch 18, Batch  1000] loss: 1.868\n",
      "[Epoch 18, Batch  1500] loss: 1.960\n",
      "[17] acc:74.99%\n",
      "[Epoch 19, Batch   500] loss: 1.620\n",
      "[Epoch 19, Batch  1000] loss: 1.727\n",
      "[Epoch 19, Batch  1500] loss: 1.819\n",
      "[18] acc:75.18%\n",
      "[Epoch 20, Batch   500] loss: 1.473\n",
      "[Epoch 20, Batch  1000] loss: 1.625\n",
      "[Epoch 20, Batch  1500] loss: 1.736\n",
      "[19] acc:74.45%\n",
      "[Epoch 21, Batch   500] loss: 1.421\n",
      "[Epoch 21, Batch  1000] loss: 1.519\n",
      "[Epoch 21, Batch  1500] loss: 1.610\n",
      "[20] acc:75.31%\n",
      "[Epoch 22, Batch   500] loss: 1.348\n",
      "[Epoch 22, Batch  1000] loss: 1.453\n",
      "[Epoch 22, Batch  1500] loss: 1.516\n",
      "[21] acc:75.02%\n",
      "[Epoch 23, Batch   500] loss: 1.250\n",
      "[Epoch 23, Batch  1000] loss: 1.345\n",
      "[Epoch 23, Batch  1500] loss: 1.417\n",
      "[22] acc:75.08%\n",
      "[Epoch 24, Batch   500] loss: 1.119\n",
      "[Epoch 24, Batch  1000] loss: 1.238\n",
      "[Epoch 24, Batch  1500] loss: 1.318\n",
      "[23] acc:75.19%\n",
      "[Epoch 25, Batch   500] loss: 1.072\n",
      "[Epoch 25, Batch  1000] loss: 1.207\n",
      "[Epoch 25, Batch  1500] loss: 1.272\n",
      "[24] acc:74.9%\n",
      "[Epoch 26, Batch   500] loss: 1.013\n",
      "[Epoch 26, Batch  1000] loss: 1.116\n",
      "[Epoch 26, Batch  1500] loss: 1.160\n",
      "[25] acc:75.06%\n",
      "[Epoch 27, Batch   500] loss: 0.925\n",
      "[Epoch 27, Batch  1000] loss: 1.007\n",
      "[Epoch 27, Batch  1500] loss: 1.082\n",
      "[26] acc:75.04%\n",
      "[Epoch 28, Batch   500] loss: 0.880\n",
      "[Epoch 28, Batch  1000] loss: 0.961\n",
      "[Epoch 28, Batch  1500] loss: 1.020\n",
      "[27] acc:74.95%\n",
      "[Epoch 29, Batch   500] loss: 0.761\n",
      "[Epoch 29, Batch  1000] loss: 0.848\n",
      "[Epoch 29, Batch  1500] loss: 0.956\n",
      "[28] acc:74.64%\n",
      "[Epoch 30, Batch   500] loss: 0.726\n",
      "[Epoch 30, Batch  1000] loss: 0.794\n",
      "[Epoch 30, Batch  1500] loss: 0.880\n",
      "[29] acc:74.71%\n",
      "[Epoch 31, Batch   500] loss: 0.684\n",
      "[Epoch 31, Batch  1000] loss: 0.736\n",
      "[Epoch 31, Batch  1500] loss: 0.827\n",
      "[30] acc:74.11%\n",
      "[Epoch 32, Batch   500] loss: 0.613\n",
      "[Epoch 32, Batch  1000] loss: 0.697\n",
      "[Epoch 32, Batch  1500] loss: 0.748\n",
      "[31] acc:74.09%\n",
      "[Epoch 33, Batch   500] loss: 0.621\n",
      "[Epoch 33, Batch  1000] loss: 0.664\n",
      "[Epoch 33, Batch  1500] loss: 0.723\n",
      "[32] acc:74.79%\n",
      "[Epoch 34, Batch   500] loss: 0.557\n",
      "[Epoch 34, Batch  1000] loss: 0.600\n",
      "[Epoch 34, Batch  1500] loss: 0.679\n",
      "[33] acc:74.88%\n",
      "[Epoch 35, Batch   500] loss: 0.505\n",
      "[Epoch 35, Batch  1000] loss: 0.571\n",
      "[Epoch 35, Batch  1500] loss: 0.595\n",
      "[34] acc:74.28%\n",
      "[Epoch 36, Batch   500] loss: 0.444\n",
      "[Epoch 36, Batch  1000] loss: 0.508\n",
      "[Epoch 36, Batch  1500] loss: 0.557\n",
      "[35] acc:74.12%\n",
      "[Epoch 37, Batch   500] loss: 0.405\n",
      "[Epoch 37, Batch  1000] loss: 0.515\n",
      "[Epoch 37, Batch  1500] loss: 0.532\n",
      "[36] acc:74.12%\n",
      "[Epoch 38, Batch   500] loss: 0.396\n",
      "[Epoch 38, Batch  1000] loss: 0.438\n",
      "[Epoch 38, Batch  1500] loss: 0.487\n",
      "[37] acc:74.07%\n",
      "[Epoch 39, Batch   500] loss: 0.360\n",
      "[Epoch 39, Batch  1000] loss: 0.396\n",
      "[Epoch 39, Batch  1500] loss: 0.458\n",
      "[38] acc:73.7%\n",
      "[Epoch 40, Batch   500] loss: 0.359\n",
      "[Epoch 40, Batch  1000] loss: 0.387\n",
      "[Epoch 40, Batch  1500] loss: 0.418\n",
      "[39] acc:74.39%\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Training...\")\n",
    "for epoch in range(1, 40):\n",
    "    # 用一个变量来记录每100个batch的平均loss\n",
    "    loss100 = 0.0\n",
    "    # 使用dataloader操作数据\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        # inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss100 += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
    "                  (epoch, i + 1, loss100 / 100))\n",
    "            loss100 = 0.0\n",
    "\n",
    "    evaluate(epoch)\n",
    "\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRjrqmsCvZah"
   },
   "outputs": [],
   "source": [
    "# 首先要把梯度清零，不然PyTorch每次计算梯度会累加，不清零的话第二次算的梯度等于第一次加第二次的       \n",
    "optimizer.zero_grad()\n",
    "# 计算前向传播的输出\n",
    "outputs = net(inputs)\n",
    "# 根据输出计算loss\n",
    "loss = criterion(outputs, labels)\n",
    "# 算完loss之后进行反向梯度传播，这个过程之后梯度会记录在变量中\n",
    "loss.backward()\n",
    "# 用计算的梯度去做优化\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qpzFO-5MvZ9x",
    "outputId": "84a50793-3102-46ea-e06c-8368df69cf5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 74 %\n"
     ]
    }
   ],
   "source": [
    "# 构造测试的dataloader\n",
    "dataiter = iter(testloader)\n",
    "# 预测正确的数量和总数量\n",
    "correct = 0\n",
    "total = 0\n",
    "# 使用torch.no_grad的话在前向传播中不记录梯度，节省内存\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        #images, labels = images.to(device), labels.to(device)\n",
    "        # 预测\n",
    "        outputs = net(images)\n",
    "        # 我们的网络输出的实际上是个概率分布，去最大概率的哪一项作为预测分类\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iH0YrePaWQum"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ImageClassification-test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
